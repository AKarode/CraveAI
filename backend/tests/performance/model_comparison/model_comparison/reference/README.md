# Model Comparison Reference Files

This directory contains the original model comparison implementation files and artifacts for reference purposes. These were previously located in the backend root directory but have been moved here to keep the repository organized.

## Original Files

- `model_comparison.py`: The original script that performed the model comparison
- `MODEL_COMPARISON.md`: Original documentation of the comparison methodology
- `model_comparison_report.json`: Sample output report from a previous comparison run
- `images/`: Visualizations generated by the original script

## Current Implementation

The current model comparison tests are implemented in a more modular and maintainable way:

- `tests/model_comparison/test_model_comparison.py`: The main test implementation
- `tests/model_comparison/model_utils.py`: Common utilities for model comparison
- `tests/model_comparison/run_model_comparison.py`: Script to run the comparison tests

## Improvements

The new implementation includes several improvements over the original:

1. **Proper test structure**: Follows unittest standards for integration with test runners
2. **Expanded test data**: More queries and menu items for broader evaluation
3. **Ground truth evaluation**: Measures accuracy against predefined correct answers
4. **Real consistency calculation**: Computes ranking consistency across similar queries
5. **Statistical significance testing**: Determines if performance differences are statistically significant
6. **Better visualization**: Additional visualization for recommendation accuracy
7. **Detailed reporting**: Enhanced JSON report with more comparison metrics

## Usage

The reference files should not be used directly. Instead, use the current implementation as described in the main README. 