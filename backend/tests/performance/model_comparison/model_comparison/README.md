# Model Comparison Test Module

This module provides a standardized way to compare the performance of different embedding models used in the CraveAI recommendation system.

## Overview

The test compares two embedding models:
- Our custom-trained BERT model (`MLmodel/best_model.pt`)
- OpenAI's embeddings API (used in production)

The test evaluates key performance metrics including:
1. Response time
2. Similarity score distributions
3. Ranking consistency across similar queries

## Directory Structure

```
model_comparison/
├── README.md                     # This documentation file
├── test_model_comparison.py      # Main test implementation
├── model_utils.py                # Utility functions for the tests
├── cheesecake.pdf                # Real menu file for testing
└── output/                       # Generated visualizations and reports (created during test run)
    ├── model_comparison_report.json  # Detailed metrics in JSON format
    ├── response_times.png        # Bar chart comparing response times
    ├── similarity_distributions.png  # Histograms of similarity distributions
    └── ranking_consistency.png   # Visualization of ranking consistency
```

## Requirements

- Make sure the `.env` file in the backend root contains valid OpenAI API keys
- Required Python packages are in the main project's `requirements.txt`

## Running the Tests

### Standard Tests

1. Ensure you're in the backend directory
2. Run the test directly using pytest:
   ```
   python -m pytest tests/performance/model_comparison/model_comparison/test_model_comparison.py -v
   ```

## Interpreting the Results

The test generates several output files in the `output/` directory:
- `response_times.png`: Shows average response times for each model
- `similarity_distributions.png`: Compares how similarity scores are distributed
- `ranking_consistency.png`: Shows which model is more consistent in rankings
- `model_comparison_report.json`: Contains detailed metrics for further analysis

## Extending the Tests

To add more test cases or modify the existing ones, edit `test_model_comparison.py` to add new test methods.

## Using for Presentations

The visualizations generated by this test module can be used directly in presentations to demonstrate the performance differences between embedding models, helping stakeholders understand why certain models are preferred for production.

## Troubleshooting

### Common Warnings

#### Parameter Name Warnings

```
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
```

These warnings come from the Transformers library and can be safely ignored. They're related to the internal parameter naming conventions in the BERT model.

#### PyTorch Load Warning

```
FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value)...
```

This is a future warning from PyTorch about the default behavior of `torch.load` changing in future releases. To fix it, update the `load_bert_model` function in `model_utils.py` to use `weights_only=True` when appropriate. 